# ğŸ¤– RÃ©sumeur d'Articles Wikipedia - Pipeline NLP

Ce projet implÃ©mente un systÃ¨me de rÃ©sumÃ© automatique d'articles Wikipedia utilisant des techniques avancÃ©es de traitement du langage naturel (NLP). Il combine un preprocessing sophistiquÃ© avec un modÃ¨le Transformer fine-tunÃ© pour gÃ©nÃ©rer des rÃ©sumÃ©s de qualitÃ©.

## ğŸ“‹ Table des matiÃ¨res

- [ğŸš€ Installation](#-installation)
- [ğŸ”§ Preprocessing en dÃ©tail](#-preprocessing-en-dÃ©tail)
- [ğŸ“Š Construction du dataset d'entraÃ®nement](#-construction-du-dataset-dentraÃ®nement)
- [ğŸ¯ EntraÃ®nement du modÃ¨le](#-entraÃ®nement-du-modÃ¨le)
- [ğŸ“ RÃ©sumÃ© d'articles avec le modÃ¨le](#-rÃ©sumÃ©-darticles-avec-le-modÃ¨le)
- [ğŸ“ Structure du projet](#-structure-du-projet)
- [ğŸ› ï¸ Utilisation avancÃ©e](#ï¸-utilisation-avancÃ©e)

## ğŸš€ Installation

### PrÃ©requis
- Python 3.8+
- GPU recommandÃ© pour l'entraÃ®nement (optionnel)

### 1. Cloner le projet
```bash
git clone <url-du-repo>
cd NLP
```

### 2. Installer les dÃ©pendances Python
```bash
pip install -r requirements.txt
```

### 3. Installer les modÃ¨les spaCy
```bash
python main.py --mode install_spacy
```

### 4. TÃ©lÃ©charger les ressources NLTK
Les ressources NLTK se tÃ©lÃ©chargent automatiquement au premier lancement.

## ğŸ”§ Preprocessing en dÃ©tail

Le module de preprocessing (`src/text_preprocessor.py`) implÃ©mente un pipeline sophistiquÃ© inspirÃ© des techniques du TP Pipeline NLP.

### Visualisation du preprocessing

Pour voir le preprocessing en action Ã©tape par Ã©tape :

```bash
# Visualisation interactive du preprocessing
python main.py --mode visualize

# Ou pour un article spÃ©cifique
python main.py --mode visualize --article "Intelligence artificielle" --lang fr
```

### Ã‰tapes du preprocessing

#### 1. **Nettoyage du texte Wikipedia**
- âœ… Suppression des rÃ©fÃ©rences `[1]`, `[2]`, etc.
- âœ… Suppression des balises de formatage `{{...}}`
- âœ… Suppression des URLs â†’ remplacÃ©es par `URL_TOKEN`
- âœ… Suppression des numÃ©ros de tÃ©lÃ©phone â†’ remplacÃ©s par `PHONE_TOKEN`
- âœ… Suppression des montants d'argent â†’ remplacÃ©s par `MONEY_TOKEN`
- âœ… Normalisation de la ponctuation excessive

#### 2. **Tokenisation**
- SÃ©paration en phrases avec `sent_tokenize()`
- SÃ©paration en mots avec `word_tokenize()`

#### 3. **Suppression des mots vides intelligente**
- Supprime les stop words classiques
- **PrÃ©serve les mots de liaison importants** : `donc`, `mais`, `car`, `ainsi`, etc.
- AdaptÃ©e pour franÃ§ais et anglais

#### 4. **Lemmatisation**
- Utilise TextBlob pour la lemmatisation
- RÃ©duit les mots Ã  leur forme canonique
- AmÃ©liore la qualitÃ© du preprocessing

#### 5. **Extraction de features**
```python
features = {
    'caps_ratio': ratio_majuscules,
    'sentence_count': nombre_phrases,
    'avg_sentence_length': longueur_moyenne_phrases,
    'word_count': nombre_mots,
    'lexical_diversity': ratio_mots_uniques,
    'sentence_scores': scores_qualitÃ©_phrases
}
```

### Configuration du preprocessing

Le preprocessing est configurable via `src/config.py` :

```python
PREPROCESSING_CONFIG = {
    'remove_urls': True,
    'remove_phone_numbers': True,  
    'remove_money_mentions': True,
    'convert_to_lowercase': True,
    'remove_punctuation': True,
    'remove_stopwords': True,
    'lemmatize': True
}
```

## ğŸ“Š Construction du dataset d'entraÃ®nement

### Processus automatisÃ©

Le dataset est construit automatiquement via le mode `build_dataset` :

```bash
# Construction d'un dataset de 100 articles franÃ§ais
python main.py --mode build_dataset --num_articles 100 --lang fr

# Pour l'anglais
python main.py --mode build_dataset --num_articles 50 --lang en
```

### Ce qui se passe lors de la construction

1. **RÃ©cupÃ©ration d'articles Wikipedia alÃ©atoires**
   - Utilise l'API Wikipedia
   - Filtre par longueur (min: 1000, max: 10000 caractÃ¨res)
   - GÃ¨re les erreurs et articles manquants

2. **PrÃ©processing complet**
   - Application du pipeline de nettoyage
   - Extraction de toutes les features numÃ©riques
   - Scoring des phrases pour le rÃ©sumÃ©

3. **GÃ©nÃ©ration des rÃ©sumÃ©s de rÃ©fÃ©rence**
   - Utilise les rÃ©sumÃ©s Wikipedia originaux
   - Nettoyage et normalisation
   - Limitation Ã  100-500 caractÃ¨res

4. **Sauvegarde structurÃ©e**
   ```
   data/wikipedia_dataset_fr.csv
   â”œâ”€â”€ original_content      # Texte brut
   â”œâ”€â”€ processed_content     # Texte aprÃ¨s preprocessing
   â”œâ”€â”€ wikipedia_summary     # RÃ©sumÃ© Wikipedia original
   â”œâ”€â”€ target_summary        # RÃ©sumÃ© cible nettoyÃ©
   â”œâ”€â”€ sentence_count        # Nombre de phrases
   â”œâ”€â”€ lexical_diversity     # DiversitÃ© lexicale
   â”œâ”€â”€ compression_ratio     # Ratio de compression
   â””â”€â”€ ... autres features
   ```

### Structure du dataset final

Chaque ligne contient :
- **Texte original ET texte preprocessÃ©**
- **Toutes les features numÃ©riques extraites**
- **MÃ©tadonnÃ©es complÃ¨tes** (URL, langue, longueurs)
- **Scores de qualitÃ©** de chaque phrase
- **RÃ©sumÃ© de rÃ©fÃ©rence** nettoyÃ©

## ğŸ¯ EntraÃ®nement du modÃ¨le

### Lancement de l'entraÃ®nement

```bash
# EntraÃ®nement complet automatique
python main.py --mode train
```

### Processus d'entraÃ®nement

#### 1. **Chargement et prÃ©paration des donnÃ©es**
- Lecture du dataset CSV
- Nettoyage des donnÃ©es manquantes
- Division train/test (80%/20%)

#### 2. **Configuration du modÃ¨le**
- **ModÃ¨le de base** : `moussaKam/barthez` (BART optimisÃ© pour le franÃ§ais)
- **Tokenizer** : SentencePiece avec tokens spÃ©ciaux
- **Architecture** : Seq2Seq Transformer

#### 3. **ParamÃ¨tres d'entraÃ®nement**
```python
training_args = {
    'learning_rate': 3e-5,
    'batch_size': 4,
    'num_epochs': 3,
    'max_input_length': 512,
    'max_output_length': 128,
    'fp16': True,  # Si GPU disponible
    'eval_strategy': 'epoch'
}
```

#### 4. **EntraÃ®nement avec validation**
- Sauvegarde automatique des checkpoints
- Monitoring des mÃ©triques (loss, BLEU, ROUGE)
- Early stopping basÃ© sur la validation

#### 5. **Sauvegarde du modÃ¨le**
```
models/summarization_wikipedia/
â”œâ”€â”€ config.json
â”œâ”€â”€ model.safetensors
â”œâ”€â”€ tokenizer.json
â”œâ”€â”€ training_args.bin
â””â”€â”€ checkpoint-*/
```

### Monitoring de l'entraÃ®nement

- **Logs** : Affichage temps rÃ©el des mÃ©triques
- **Tensorboard** : Graphiques de progression
- **Checkpoints** : Sauvegarde toutes les epochs

## ğŸ“ RÃ©sumÃ© d'articles avec le modÃ¨le

### Modes de rÃ©sumÃ© disponibles

#### 1. **Mode ligne de commande**
```bash
# RÃ©sumÃ© d'un article spÃ©cifique avec le modÃ¨le entraÃ®nÃ©
python main.py --mode summarize --article "Intelligence artificielle" --lang fr

# Avec paramÃ¨tres personnalisÃ©s
python main.py --mode summarize --article "Machine learning" --lang fr --max_length 150 --min_length 50
```

#### 2. **Mode interactif**
```bash
# Interface interactive complÃ¨te
python main.py --mode interactive
```

#### 3. **Mode dÃ©monstration**
```bash
# DÃ©monstration avec articles prÃ©dÃ©finis
python main.py --mode demo
```

### FonctionnalitÃ©s du rÃ©sumÃ©

- **ModÃ¨le Transformer entraÃ®nÃ©** (pas de rÃ©sumÃ© extractif basique)
- **ParamÃ¨tres configurables** : longueur min/max, beam search
- **Support multi-langue** : franÃ§ais et anglais
- **MÃ©triques de qualitÃ©** : compression ratio, longueurs
- **Interface intuitive** avec affichage formatÃ©

### Utilisation programmatique

```python
import sys
sys.path.append('src')

from src.wikipedia_summarizer_trained import WikipediaSummarizerTrained

# Initialisation
summarizer = WikipediaSummarizerTrained("./models/summarization_wikipedia")

# RÃ©sumÃ© d'un article
result = summarizer.summarize_article("Machine learning", language='fr')

print(f"Titre: {result['title']}")
print(f"RÃ©sumÃ©: {result['summary']}")
print(f"Compression: {result['compression_ratio']:.2%}")
```

### ParamÃ¨tres de gÃ©nÃ©ration

Le modÃ¨le supporte diffÃ©rents paramÃ¨tres :

```python
summary = summarizer.generate_summary(
    text,
    max_length=128,    # Longueur max du rÃ©sumÃ©
    min_length=30,     # Longueur min du rÃ©sumÃ©  
    num_beams=4,       # Beam search
    temperature=1.0,   # CrÃ©ativitÃ©
    length_penalty=1.0 # PÃ©nalitÃ© longueur
)
```

## ğŸ“ Structure du projet

```
NLP/
â”œâ”€â”€ ğŸ“„ main.py                    # Point d'entrÃ©e unifiÃ© pour tous les modes
â”œâ”€â”€ ğŸ“„ requirements.txt           # DÃ©pendances Python
â”œâ”€â”€ ğŸ“„ build_steps.txt           # Documentation technique
â”‚
â”œâ”€â”€ ğŸ“ src/                      # Modules principaux
â”‚   â”œâ”€â”€ ğŸ“„ config.py             # Configuration globale
â”‚   â”œâ”€â”€ ğŸ“„ dataset_builder.py    # Construction dataset
â”‚   â”œâ”€â”€ ğŸ“„ text_preprocessor.py  # Pipeline preprocessing
â”‚   â”œâ”€â”€ ğŸ“„ wikipedia_summarizer_trained.py # RÃ©sumeur avec modÃ¨le entraÃ®nÃ©
â”‚   â”œâ”€â”€ ğŸ“„ train_summarization_model.py    # Script d'entraÃ®nement
â”‚   â”œâ”€â”€ ğŸ“„ visualize_preprocessing.py      # Visualisation preprocessing
â”‚   â””â”€â”€ ğŸ“„ install_spacy_models.py         # Installation modÃ¨les spaCy
â”‚
â”œâ”€â”€ ğŸ“ data/                     # Datasets
â”‚   â””â”€â”€ ğŸ“„ wikipedia_dataset_fr.csv
â”‚
â”œâ”€â”€ ğŸ“ models/                   # ModÃ¨les entraÃ®nÃ©s
â”‚   â””â”€â”€ ğŸ“ summarization_wikipedia/
â”‚       â”œâ”€â”€ ğŸ“„ config.json
â”‚       â”œâ”€â”€ ğŸ“„ model.safetensors
â”‚       â””â”€â”€ ğŸ“ checkpoint-*/
â”‚
â””â”€â”€ ğŸ“ wandb/                    # Logs d'entraÃ®nement
```

## ğŸ› ï¸ Utilisation avancÃ©e

### Pipeline complet d'utilisation

```bash
# 1. Installation des prÃ©requis
python main.py --mode install_spacy

# 2. Construction du dataset
python main.py --mode build_dataset --num_articles 200 --lang fr

# 3. EntraÃ®nement du modÃ¨le
python main.py --mode train

# 4. Ã‰valuation du modÃ¨le
python main.py --mode evaluate

# 5. Utilisation du modÃ¨le
python main.py --mode interactive
```

### Modes disponibles

| Mode | Description | Exemple |
|------|-------------|---------|
| `install_spacy` | Installation modÃ¨les spaCy | `python main.py --mode install_spacy` |
| `build_dataset` | Construction dataset | `python main.py --mode build_dataset --num_articles 100` |
| `train` | EntraÃ®nement modÃ¨le | `python main.py --mode train` |
| `summarize` | RÃ©sumÃ© article | `python main.py --mode summarize --article "IA"` |
| `interactive` | Mode interactif | `python main.py --mode interactive` |
| `demo` | DÃ©monstration | `python main.py --mode demo` |
| `visualize` | Visualisation preprocessing | `python main.py --mode visualize` |
| `evaluate` | Ã‰valuation modÃ¨le | `python main.py --mode evaluate` |

### Personnalisation du preprocessing

Modifiez `src/config.py` pour adapter le preprocessing :

```python
PREPROCESSING_CONFIG = {
    'remove_urls': False,         # Garder les URLs
    'lemmatize': False,          # DÃ©sactiver lemmatisation
    'min_sentence_length': 10    # Filtrer phrases courtes
}
```

### EntraÃ®nement avec plus de donnÃ©es

```bash
# Dataset plus large pour de meilleures performances
python main.py --mode build_dataset --num_articles 1000 --lang fr
```

### Utilisation multi-langue

Le systÃ¨me supporte franÃ§ais et anglais :

```bash
# Dataset anglais
python main.py --mode build_dataset --lang en --num_articles 200

# RÃ©sumÃ© en anglais
python main.py --mode summarize --article "Artificial intelligence" --lang en
```

---

## ğŸ¯ RÃ©sultats attendus

- **RÃ©sumÃ©s cohÃ©rents** de 30-128 mots
- **Compression intelligente** (ratio 5-15%)
- **PrÃ©servation du sens** principal de l'article
- **QualitÃ© linguistique** grÃ¢ce au preprocessing avancÃ©

## ğŸ”§ DiffÃ©rences importantes

### âš ï¸ **Attention : Deux types de rÃ©sumÃ©**

1. **RÃ©sumÃ© pour dataset** (construction) : Utilise les rÃ©sumÃ©s Wikipedia originaux nettoyÃ©s
2. **RÃ©sumÃ© final** (utilisation) : Utilise le modÃ¨le Transformer entraÃ®nÃ©

Le mode `summarize` utilise le **modÃ¨le entraÃ®nÃ©**, pas les rÃ©sumÃ©s Wikipedia !

## ğŸ”§ DÃ©pannage

### ProblÃ¨mes courants

1. **ModÃ¨le non trouvÃ©** : VÃ©rifiez que l'entraÃ®nement s'est bien terminÃ© avec `python main.py --mode train`
2. **Erreur CUDA** : Le systÃ¨me dÃ©tecte automatiquement GPU/CPU
3. **Article non trouvÃ©** : VÃ©rifiez l'orthographe et la langue
4. **Dataset manquant** : Construisez d'abord le dataset avec `--mode build_dataset`

### Support

Pour plus d'informations, consultez les logs dÃ©taillÃ©s ou les fichiers de configuration dans `src/`. 