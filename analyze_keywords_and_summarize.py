#!/usr/bin/env python3
"""
üîç ANALYSE DES MOTS-CL√âS ET G√âN√âRATION DE R√âSUM√â AUTOMATIQUE

Ce module analyse les termes TF-IDF les plus importants d'un article Wikipedia
et g√©n√®re un r√©sum√© automatique bas√© sur les phrases contenant le plus de mots-cl√©s.

Pipeline:
1. R√©cup√©ration de l'article Wikipedia
2. Preprocessing avec le m√™me pipeline que l'entra√Ænement
3. Pr√©diction du cluster avec les mod√®les entra√Æn√©s
4. Analyse TF-IDF des termes importants
5. Filtrage des r√©f√©rences bibliographiques
6. Scoring des phrases par pertinence
7. G√©n√©ration du r√©sum√© final

Usage:
    python analyze_keywords_and_summarize.py "Titre Article"
    
Ou depuis main.py:
    python main.py --mode analyze --article "Titre Article"
"""

import sys
import numpy as np
import pandas as pd
import pickle
import logging
from pathlib import Path
from collections import Counter
import re

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Ajout du path
sys.path.append('src')

def get_wikipedia_article_direct(title, language='fr'):
    """R√©cup√®re un article Wikipedia"""
    try:
        import wikipediaapi
        
        wiki = wikipediaapi.Wikipedia(
            user_agent='NLP-Pipeline-Analysis/1.0 (Educational use)',
            language=language
        )
        
        print(f"üîç Recherche de l'article: '{title}'...")
        
        page = wiki.page(title)
        
        if not page.exists():
            print(f"‚ùå Article '{title}' non trouv√©")
            
            # Suggestions d'articles populaires
            suggestions = [
                'Intelligence artificielle',
                'Python (langage)',
                'Machine learning',
                'France',
                'Informatique',
                'Chimie',
                'Physique',
                'Math√©matiques'
            ]
            
            print(f"üí° Suggestions d'articles:")
            for suggestion in suggestions:
                print(f"   - {suggestion}")
            
            return None
        
        print(f"‚úÖ Article trouv√©: {page.title}")
        print(f"üìÑ Longueur: {len(page.text):,} caract√®res")
        print(f"üîó URL: {page.fullurl}")
        
        return {
            'title': page.title,
            'text': page.text,
            'url': page.fullurl,
            'length': len(page.text)
        }
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la r√©cup√©ration: {e}")
        return None

def preprocess_article_direct(article_text, language='fr'):
    """Preprocess l'article avec le m√™me pipeline que le dataset"""
    try:
        from src.text_preprocessor import TextPreprocessor
        from src.config import Config
        
        print("üßπ Preprocessing de l'article...")
        
        # Configuration identique au dataset
        config = Config()
        preprocessor = TextPreprocessor(config.PREPROCESSING_CONFIG)
        
        # Preprocessing complet
        clean_text, features = preprocessor.preprocessing_pipeline(article_text, language)
        
        print(f"‚úÖ Preprocessing termin√©")
        print(f"   üìä Texte original: {len(article_text):,} caract√®res")
        print(f"   üìä Texte nettoy√©: {len(clean_text):,} caract√®res")
        print(f"   üìä Features extraites: {len(features)} √©l√©ments")
        
        return clean_text, features
        
    except Exception as e:
        print(f"‚ùå Erreur preprocessing: {e}")
        return None, None

def predict_with_pipeline_direct(clean_text):
    """Pr√©dit le cluster avec le pipeline entra√Æn√©"""
    try:
        from src.autoencoder_clustering import AutoencoderClustering
        
        print("ü§ñ Chargement des mod√®les...")
        
        # Configuration (identique √† l'entra√Ænement)
        config = {
            'max_features': 5000,
            'encoding_dim': 32,
            'n_clusters': 5,
            'random_state': 42,
            'models_dir': 'models'
        }
        
        # Initialisation et chargement
        pipeline = AutoencoderClustering(config)
        
        if not pipeline.load_models():
            print("‚ùå Impossible de charger les mod√®les")
            print("üí° Assurez-vous d'avoir ex√©cut√©: python main.py --mode train2")
            return None
        
        print("‚úÖ Mod√®les charg√©s avec succ√®s")
        
        # Pipeline complet de pr√©diction
        print("üî§ Vectorisation TF-IDF...")
        tfidf_vector = pipeline.tfidf_vectorizer.transform([clean_text]).toarray()
        
        print("üß† Encodage en 32 dimensions...")
        encoded_vector = pipeline.encoder.predict(tfidf_vector, verbose=0)
        
        print("üéØ Pr√©diction du cluster...")
        predicted_cluster = pipeline.kmeans.predict(encoded_vector)[0]
        
        # Distance aux centres des clusters
        cluster_distances = pipeline.kmeans.transform(encoded_vector)[0]
        
        # Probabilit√©s relatives (plus la distance est petite, plus c'est probable)
        cluster_probs = 1 / (1 + cluster_distances)
        cluster_probs = cluster_probs / cluster_probs.sum()
        
        results = {
            'predicted_cluster': predicted_cluster,
            'tfidf_vector': tfidf_vector,
            'encoded_vector': encoded_vector,
            'cluster_distances': cluster_distances,
            'cluster_probabilities': cluster_probs
        }
        
        return results
        
    except Exception as e:
        print(f"‚ùå Erreur pr√©diction: {e}")
        return None

def analyze_article_keywords(article_title="Chimie"):
    """Analyse les mots-cl√©s les plus importants d'un article"""
    
    print("üîç ANALYSE DES MOTS-CL√âS ET G√âN√âRATION DE R√âSUM√â")
    print("="*70)
    
    try:
        print(f"üì∞ Analyse de l'article: {article_title}")
        
        # 1. R√©cup√©ration et preprocessing de l'article
        article = get_wikipedia_article_direct(article_title, 'fr')
        if not article:
            return
        
        # 2. Preprocessing
        clean_text, features = preprocess_article_direct(article['text'], 'fr')
        if not clean_text:
            return
        
        # 3. Pr√©diction
        results = predict_with_pipeline_direct(clean_text)
        if not results:
            return
        
        predicted_cluster = results['predicted_cluster']
        print(f"üéØ Article class√© dans le cluster: {predicted_cluster}")
        
        # 2. Analyse TF-IDF d√©taill√©e
        print(f"\nüìä ANALYSE TF-IDF:")
        analyze_tfidf_importance(clean_text, results)
        
        # 3. Analyse du cluster
        print(f"\nüß† ANALYSE DU CLUSTER {predicted_cluster}:")
        analyze_cluster_characteristics(predicted_cluster)
        
        # 4. Extraction des phrases importantes
        print(f"\nüìù EXTRACTION DES PHRASES IMPORTANTES:")
        important_sentences = extract_important_sentences(article['text'], clean_text, results)
        
        # 5. G√©n√©ration du r√©sum√©
        print(f"\n‚ú® G√âN√âRATION DU R√âSUM√â:")
        generate_summary(article, important_sentences, results)
        
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        raise

def analyze_tfidf_importance(clean_text, results):
    """Analyse l'importance des termes TF-IDF"""
    try:
        # Chargement du vectoriseur
        with open('models/tfidf_vectorizer.pkl', 'rb') as f:
            tfidf_vectorizer = pickle.load(f)
        
        # Vecteur TF-IDF de l'article
        tfidf_vector = results['tfidf_vector'][0]
        
        # R√©cup√©ration du vocabulaire
        feature_names = tfidf_vectorizer.get_feature_names_out()
        
        # Tri des termes par importance TF-IDF
        term_scores = list(zip(feature_names, tfidf_vector))
        term_scores = sorted(term_scores, key=lambda x: x[1], reverse=True)
        
        # Top 20 termes les plus importants
        print(f"üî§ TOP 20 TERMES TF-IDF:")
        for i, (term, score) in enumerate(term_scores[:20]):
            if score > 0:
                print(f"   {i+1:2d}. {term:<15} : {score:.4f}")
        
        # Statistiques
        non_zero_terms = sum(1 for _, score in term_scores if score > 0)
        print(f"\nüìä Statistiques TF-IDF:")
        print(f"   - Termes non-z√©ros: {non_zero_terms:,}")
        print(f"   - Score max: {tfidf_vector.max():.4f}")
        print(f"   - Score moyen: {tfidf_vector[tfidf_vector > 0].mean():.4f}")
        
        return term_scores[:50]  # Retourner top 50 pour l'analyse
        
    except Exception as e:
        print(f"‚ùå Erreur analyse TF-IDF: {e}")
        return []

def analyze_cluster_characteristics(cluster_id):
    """Analyse les caract√©ristiques du cluster"""
    try:
        # Chargement des r√©sultats d'entra√Ænement
        with open('models/pipeline_results.pkl', 'rb') as f:
            training_results = pickle.load(f)
        
        # Articles du m√™me cluster dans le dataset d'entra√Ænement
        cluster_labels = training_results['cluster_labels']
        cluster_mask = cluster_labels == cluster_id
        
        # Vecteurs encod√©s du cluster
        encoded_vectors = training_results['encoded_vectors']
        cluster_vectors = encoded_vectors[cluster_mask]
        
        print(f"üìà Caract√©ristiques du cluster {cluster_id}:")
        print(f"   - Nombre d'articles: {cluster_mask.sum()}")
        print(f"   - Vecteur moyen: [{cluster_vectors.mean(axis=0)[:5].round(3)}...]")
        print(f"   - Std moyen: {cluster_vectors.std(axis=0).mean():.3f}")
        
        # Centre du cluster K-means
        with open('models/kmeans_model.pkl', 'rb') as f:
            kmeans = pickle.load(f)
        
        cluster_center = kmeans.cluster_centers_[cluster_id]
        print(f"   - Centre K-means: [{cluster_center[:5].round(3)}...]")
        
        return cluster_vectors.mean(axis=0)
        
    except Exception as e:
        print(f"‚ùå Erreur analyse cluster: {e}")
        return None

def extract_important_sentences(original_text, clean_text, results):
    """Extrait les phrases les plus importantes bas√©es sur les mots-cl√©s TF-IDF"""
    try:
        from nltk.tokenize import sent_tokenize
        
        # Chargement du vectoriseur
        with open('models/tfidf_vectorizer.pkl', 'rb') as f:
            tfidf_vectorizer = pickle.load(f)
        
        # R√©cup√©ration des termes importants
        tfidf_vector = results['tfidf_vector'][0]
        feature_names = tfidf_vectorizer.get_feature_names_out()
        
        # Top mots-cl√©s (score > seuil)
        threshold = np.percentile(tfidf_vector[tfidf_vector > 0], 80)  # Top 20%
        important_terms = []
        
        for term, score in zip(feature_names, tfidf_vector):
            if score >= threshold:
                important_terms.append((term, score))
        
        important_terms = sorted(important_terms, key=lambda x: x[1], reverse=True)
        
        print(f"üéØ Mots-cl√©s s√©lectionn√©s (score > {threshold:.4f}):")
        for term, score in important_terms[:10]:
            print(f"   - {term}: {score:.4f}")
        
        # Tokenisation en phrases
        sentences = sent_tokenize(original_text)
        
        # Patterns pour filtrer les r√©f√©rences bibliographiques
        bibliographic_patterns = [
            r'\b(isbn|issn)\s*:?\s*\d',  # ISBN/ISSN
            r'\b(√©d\.|√©dition|√©ditions)\b',  # √âditions
            r'\b(p\.|pp\.|page|pages)\s*\d+',  # Pages
            r'coll\.',  # Collection
            r'phonetoken',  # Tokens de t√©l√©phone
            r'et\s+al\.',  # Et alii
            r'^\s*[A-Z]\.\s*[A-Z]',  # Initiales d'auteur (d√©but de phrase)
            r'\b\d{4}\b.*\b(paris|dunod|mcgraw|lavoisier|colin)\b',  # Ann√©e + √©diteur
            r'^\s*[A-Z][a-z]+,\s*[A-Z]',  # Nom d'auteur au d√©but
            r'^\s*\([^)]*\)',  # Parenth√®ses au d√©but (souvent r√©f√©rences)
            r'cours\s+de\s+[A-Z]',  # "cours de Paul"
        ]
        
        # Score de chaque phrase bas√© sur les mots-cl√©s
        sentence_scores = []
        
        for sentence in sentences:
            # Filtrer les r√©f√©rences bibliographiques
            is_bibliography = False
            for pattern in bibliographic_patterns:
                if re.search(pattern, sentence, re.IGNORECASE):
                    is_bibliography = True
                    break
            
            # Ignorer les r√©f√©rences bibliographiques
            if is_bibliography:
                continue
            
            # Nettoyage basique de la phrase
            clean_sentence = re.sub(r'[^\w\s]', ' ', sentence.lower())
            clean_sentence = ' '.join(clean_sentence.split())
            
            # Score bas√© sur la pr√©sence des mots-cl√©s
            score = 0
            word_count = 0
            
            for term, term_score in important_terms:
                if term in clean_sentence:
                    score += term_score
                    word_count += 1
            
            # Privil√©gier les phrases avec du contenu scientifique
            science_bonus = 0
            science_terms = ['science', '√©tudie', 'mati√®re', 'atomes', 'mol√©cules', 'r√©actions', 'transformations', 'compos√©s', '√©l√©ments', 'propri√©t√©s']
            for science_term in science_terms:
                if science_term in clean_sentence:
                    science_bonus += 0.1
            
            # Normalisation par la longueur de la phrase + bonus scientifique
            if len(sentence.split()) > 8:  # √âviter les phrases trop courtes
                normalized_score = (score + science_bonus) / len(sentence.split()) * 100
                sentence_scores.append((sentence, score, normalized_score, word_count))
        
        # Tri par score normalis√©
        sentence_scores = sorted(sentence_scores, key=lambda x: x[2], reverse=True)
        
        print(f"\nüìù TOP 10 PHRASES IMPORTANTES:")
        for i, (sentence, score, norm_score, word_count) in enumerate(sentence_scores[:10]):
            print(f"\n{i+1:2d}. Score: {norm_score:.2f} ({word_count} mots-cl√©s)")
            print(f"    {sentence[:100]}{'...' if len(sentence) > 100 else ''}")
        
        return sentence_scores[:15]  # Retourner top 15 phrases
        
    except Exception as e:
        print(f"‚ùå Erreur extraction phrases: {e}")
        return []

def generate_summary(article, important_sentences, results):
    """G√©n√®re un r√©sum√© bas√© sur les phrases importantes"""
    try:
        predicted_cluster = results['predicted_cluster']
        
        print(f"üìÑ R√âSUM√â AUTOMATIQUE - {article['title']}")
        print("="*60)
        
        # S√©lection des meilleures phrases pour le r√©sum√©
        summary_sentences = []
        total_length = 0
        max_length = 800  # Longueur cible du r√©sum√©
        
        # √âviter les doublons et phrases trop similaires
        for sentence, score, norm_score, word_count in important_sentences:
            if total_length + len(sentence) <= max_length:
                # V√©rifier qu'elle n'est pas trop similaire aux pr√©c√©dentes
                is_similar = False
                for existing in summary_sentences:
                    # Simple v√©rification de similarit√©
                    common_words = set(sentence.lower().split()) & set(existing.lower().split())
                    if len(common_words) > min(len(sentence.split()), len(existing.split())) * 0.5:
                        is_similar = True
                        break
                
                if not is_similar and len(sentence.split()) >= 6:
                    summary_sentences.append(sentence)
                    total_length += len(sentence)
                    
                    if len(summary_sentences) >= 5:  # Max 5 phrases
                        break
        
        # Affichage du r√©sum√©
        if summary_sentences:
            print("\nüìã R√©sum√© g√©n√©r√©:")
            for i, sentence in enumerate(summary_sentences):
                print(f"{i+1}. {sentence}")
            
            print(f"\nüìä M√©tadonn√©es du r√©sum√©:")
            print(f"   - Longueur: {total_length} caract√®res")
            print(f"   - Phrases: {len(summary_sentences)}")
            print(f"   - Article original: {len(article['text']):,} caract√®res")
            print(f"   - Taux de compression: {(total_length/len(article['text']))*100:.1f}%")
            print(f"   - Cluster assign√©: {predicted_cluster}")
            
            # Sauvegarde du r√©sum√©
            summary_file = f"resume_{article['title'].replace(' ', '_')}.txt"
            with open(summary_file, 'w', encoding='utf-8') as f:
                f.write(f"R√âSUM√â AUTOMATIQUE - {article['title']}\n")
                f.write(f"Cluster: {predicted_cluster}\n")
                f.write(f"Source: {article['url']}\n")
                f.write("="*60 + "\n\n")
                
                for i, sentence in enumerate(summary_sentences):
                    f.write(f"{i+1}. {sentence}\n\n")
            
            print(f"üíæ R√©sum√© sauvegard√©: {summary_file}")
        else:
            print("‚ùå Impossible de g√©n√©rer un r√©sum√©")
        
    except Exception as e:
        print(f"‚ùå Erreur g√©n√©ration r√©sum√©: {e}")

def main():
    """Fonction principale"""
    
    # R√©cup√©ration du titre d'article
    if len(sys.argv) > 1:
        article_title = " ".join(sys.argv[1:])
    else:
        article_title = "Chimie"  # Par d√©faut
    
    analyze_article_keywords(article_title)

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚èπÔ∏è  Analyse interrompue")
    except Exception as e:
        print(f"\n‚ùå Erreur: {e}")
        raise 