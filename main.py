#!/usr/bin/env python3
"""
ü§ñ Pipeline NLP Complet - Auto-encodeur + Clustering + R√©sum√© Automatique
Architecture : TF-IDF ‚Üí Auto-encodeur (32D) ‚Üí K-means ‚Üí Analyse intelligente

Modes disponibles:
- build_dataset: Construction du dataset d'entra√Ænement
- train: Entra√Ænement auto-encodeur + clustering (TensorFlow)
- summarize: Analyse et r√©sum√© d'un article avec le pipeline entra√Æn√©
- interactive: Mode interactif de r√©sum√©
- demo: D√©monstration avec articles pr√©d√©finis
- visualize: Visualisation du preprocessing
- install_spacy: Installation des mod√®les spaCy
- evaluate: √âvaluation du mod√®le
"""

import argparse
import logging
import sys
from pathlib import Path

# Ajout du dossier src au path
sys.path.append('src')

from src.dataset_builder import DatasetBuilder
from src.text_preprocessor import TextPreprocessor
from src.config import Config

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def build_dataset_mode(args):
    """Mode construction du dataset"""
    logger.info(f"Construction du dataset avec {args.num_articles} articles en {args.lang}")
    
    config = Config()
    builder = DatasetBuilder(config)
    dataset_path = builder.build_dataset(
        num_articles=args.num_articles,
        language=args.lang
    )
    logger.info(f"Dataset cr√©√©: {dataset_path}")


def train_mode(args):
    """Mode entra√Ænement auto-encodeur + clustering (TensorFlow)"""
    logger.info("Entra√Ænement du pipeline Auto-encodeur + Clustering")
    
    try:
        from src.autoencoder_clustering import run_autoencoder_clustering_pipeline
        
        # Configuration selon les arguments
        config = {
            'max_features': args.max_features,
            'encoding_dim': args.encoding_dim,
            'n_clusters': args.n_clusters,
            'epochs': args.epochs,
            'batch_size': getattr(args, 'batch_size', 32),
            'random_state': 42,
            'models_dir': 'models'
        }
        
        print(f"\n{'='*80}")
        print("üß† ENTRA√éNEMENT AUTO-ENCODEUR + CLUSTERING")
        print(f"{'='*80}")
        print(f"üî§ Features TF-IDF max: {config['max_features']}")
        print(f"üß† Dimensions encodage: {config['encoding_dim']}")
        print(f"üéØ Nombre de clusters: {config['n_clusters']}")
        print(f"‚è∞ √âpoques: {config['epochs']}")
        print(f"üì¶ Batch size: {config['batch_size']}")
        print(f"{'='*80}")
        
        # V√©rification du dataset
        dataset_path = "data/wikipedia_dataset_fr.csv"
        if not Path(dataset_path).exists():
            logger.error(f"‚ùå Dataset non trouv√©: {dataset_path}")
            logger.info("üí° Construisez d'abord le dataset avec: python main.py --mode build_dataset")
            return
        
        # Suppression des anciens mod√®les si ils existent
        old_models = [
            'models/autoencoder.h5',
            'models/encoder.h5'
        ]
        
        for model_path in old_models:
            if Path(model_path).exists():
                Path(model_path).unlink()
                logger.info(f"üóëÔ∏è  Suppression ancien mod√®le: {model_path}")
        
        # Lancement de l'entra√Ænement
        logger.info("üöÄ D√©marrage de l'entra√Ænement...")
        
        results = run_autoencoder_clustering_pipeline(
            dataset_path,
            max_features=config['max_features'],
            encoding_dim=config['encoding_dim'],
            n_clusters=config['n_clusters'],
            epochs=config['epochs'],
            batch_size=config['batch_size']
        )
        
        # Affichage des r√©sultats
        print(f"\n{'='*80}")
        print("üéâ ENTRA√éNEMENT TERMIN√â AVEC SUCC√àS!")
        print(f"{'='*80}")
        
        # M√©triques
        metrics = results['clustering_metrics']
        print(f"üìä Silhouette Score: {metrics['silhouette_score']:.3f}")
        print(f"üìä Inertie: {metrics['inertia']:.1f}")
        
        # Fichiers g√©n√©r√©s
        print(f"\nüíæ Mod√®les sauvegard√©s:")
        print(f"   - models/autoencoder.weights.h5")
        print(f"   - models/encoder.weights.h5")
        print(f"   - models/autoencoder_config.pkl")
        print(f"   - models/encoder_config.pkl")
        print(f"   - models/tfidf_vectorizer.pkl")
        print(f"   - models/kmeans_model.pkl")
        print(f"   - models/pipeline_results.pkl")
        
        # Prochaines √©tapes
        print(f"\nüí° Prochaines √©tapes:")
        print(f"   - Tester: python main.py --mode analyze --article \"Titre Article\"")
        print(f"   - Analyser: python analyze_keywords_and_summarize.py \"Titre\"")
        
        logger.info("Entra√Ænement auto-encodeur termin√©")
        
    except Exception as e:
        logger.error(f"Erreur lors de l'entra√Ænement: {e}")
        raise


def summarize_mode(args):
    """Mode analyse et r√©sum√© avec le pipeline auto-encodeur"""
    if not args.article:
        logger.error("Veuillez sp√©cifier un article avec --article")
        return
    
    logger.info(f"Analyse et r√©sum√© de l'article: {args.article}")
    
    try:
        import sys
        import os
        
        # Ex√©cution du script d'analyse
        print(f"\n{'='*80}")
        print("üîç ANALYSE ET R√âSUM√â AUTOMATIQUE")
        print(f"{'='*80}")
        
        # Import du module d'analyse
        current_dir = os.getcwd()
        if current_dir not in sys.path:
            sys.path.append(current_dir)
        
        from analyze_keywords_and_summarize import analyze_article_keywords
        
        # Lancement de l'analyse
        analyze_article_keywords(args.article)
        
        print(f"\n{'='*80}")
        print("‚úÖ ANALYSE TERMIN√âE!")
        print(f"{'='*80}")
        
    except Exception as e:
        logger.error(f"Erreur lors de l'analyse: {e}")
        print("üí° Assurez-vous que les mod√®les sont entra√Æn√©s avec --mode train")
        raise


def interactive_mode(args):
    """Mode interactif de r√©sum√©"""
    logger.info("Lancement du mode interactif")
    
    print("\nüéÆ MODE INTERACTIF - R√âSUM√â D'ARTICLES")
    print("="*60)
    
    try:
        from analyze_keywords_and_summarize import analyze_article_keywords
        
        while True:
            print("\nüí¨ Entrez le titre d'un article Wikipedia √† analyser:")
            print("   (ou 'quit' pour quitter)")
            
            article_title = input("üëâ Titre: ").strip()
            
            if article_title.lower() in ['quit', 'q', 'exit']:
                print("üëã Au revoir!")
                break
            
            if not article_title:
                print("‚ùå Titre vide, veuillez r√©essayer")
                continue
            
            try:
                print(f"\nüîç Analyse de '{article_title}'...")
                analyze_article_keywords(article_title)
                print("\n" + "="*60)
                
            except KeyboardInterrupt:
                print("\n‚è∏Ô∏è  Analyse interrompue")
                break
            except Exception as e:
                print(f"‚ùå Erreur: {e}")
                print("üí° Essayez un autre article")
        
    except Exception as e:
        logger.error(f"Erreur mode interactif: {e}")


def demo_mode(args):
    """Mode d√©monstration avec articles pr√©d√©finis"""
    logger.info("Lancement de la d√©monstration")
    
    # Articles d'exemple
    demo_articles = [
        "Intelligence artificielle",
        "Python (langage)", 
        "Chimie",
        "Informatique",
        "Machine learning"
    ]
    
    print("\nüé™ D√âMONSTRATION - R√âSUM√â D'ARTICLES")
    print("="*60)
    print(f"üìö {len(demo_articles)} articles d'exemple")
    
    try:
        from analyze_keywords_and_summarize import analyze_article_keywords
        
        for i, article in enumerate(demo_articles, 1):
            print(f"\n{'='*80}")
            print(f"üìñ ARTICLE {i}/{len(demo_articles)}: {article}")
            print(f"{'='*80}")
            
            try:
                analyze_article_keywords(article)
                
                if i < len(demo_articles):
                    input("\n‚è∏Ô∏è  Appuyez sur Entr√©e pour continuer...")
                    
            except KeyboardInterrupt:
                print("\n‚èπÔ∏è  D√©monstration interrompue")
                break
            except Exception as e:
                print(f"‚ùå Erreur pour '{article}': {e}")
                continue
        
        print(f"\nüéâ D√©monstration termin√©e!")
        
    except Exception as e:
        logger.error(f"Erreur d√©monstration: {e}")


def visualize_mode(args):
    """Mode visualisation du preprocessing"""
    logger.info("Lancement de la visualisation du preprocessing")
    
    from src.visualize_preprocessing import visualize_preprocessing
    visualize_preprocessing(args.article, args.lang)


def install_spacy_mode(args):
    """Mode installation des mod√®les spaCy"""
    logger.info("Installation des mod√®les spaCy")
    
    from src.install_spacy_models import install_spacy_models
    install_spacy_models()


def evaluate_mode(args):
    """Mode √©valuation du pipeline"""
    logger.info("√âvaluation du pipeline auto-encodeur")
    
    try:
        import pickle
        import numpy as np
        from pathlib import Path
        
        print(f"\n{'='*60}")
        print("üìä √âVALUATION DU PIPELINE")
        print(f"{'='*60}")
        
        # V√©rification des mod√®les
        model_files = [
            'models/autoencoder.weights.h5',
            'models/encoder.weights.h5',
            'models/tfidf_vectorizer.pkl',
            'models/kmeans_model.pkl',
            'models/pipeline_results.pkl'
        ]
        
        missing_files = [f for f in model_files if not Path(f).exists()]
        
        if missing_files:
            print("‚ùå Mod√®les manquants:")
            for f in missing_files:
                print(f"   - {f}")
            print("üí° Ex√©cutez: python main.py --mode train")
            return
        
        # Chargement des r√©sultats
        with open('models/pipeline_results.pkl', 'rb') as f:
            results = pickle.load(f)
        
        print("‚úÖ Tous les mod√®les sont pr√©sents")
        
        # M√©triques
        metrics = results['clustering_metrics']
        tfidf_shape = results['tfidf_matrix'].shape
        encoded_shape = results['encoded_vectors'].shape
        
        print(f"\nüìä M√âTRIQUES DU PIPELINE:")
        print(f"   - Silhouette Score: {metrics['silhouette_score']:.3f}")
        print(f"   - Inertie K-means: {metrics['inertia']:.1f}")
        print(f"   - Documents trait√©s: {tfidf_shape[0]}")
        print(f"   - Compression: {tfidf_shape[1]} ‚Üí {encoded_shape[1]} dims")
        print(f"   - Taux de compression: {(encoded_shape[1]/tfidf_shape[1])*100:.1f}%")
        
        # Distribution des clusters
        cluster_labels = results['cluster_labels']
        unique, counts = np.unique(cluster_labels, return_counts=True)
        
        print(f"\nüéØ DISTRIBUTION DES CLUSTERS:")
        total = len(cluster_labels)
        for cluster_id, count in zip(unique, counts):
            percentage = (count / total) * 100
            print(f"   - Cluster {cluster_id}: {count} docs ({percentage:.1f}%)")
        
        print(f"\nüíæ Taille des mod√®les:")
        for model_file in model_files:
            if Path(model_file).exists():
                size_kb = Path(model_file).stat().st_size / 1024
                print(f"   - {model_file}: {size_kb:.1f} KB")
        
        print(f"{'='*60}")
        
    except Exception as e:
        logger.error(f"Erreur √©valuation: {e}")
        print("üí° Assurez-vous que les mod√®les sont entra√Æn√©s")





def main():
    """Point d'entr√©e principal du programme"""
    parser = argparse.ArgumentParser(
        description="R√©sumeur d'articles Wikip√©dia avec mod√®le Transformer entra√Æn√©",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples d'utilisation:
  # 1. Installer les mod√®les spaCy
  python main.py --mode install_spacy
  
  # 2. Construire un dataset
  python main.py --mode build_dataset --num_articles 50 --lang fr
  
  # 3. Entra√Æner le pipeline auto-encodeur + clustering
  python main.py --mode train --encoding_dim 32 --n_clusters 5 --epochs 100
  
  # 4. Analyser et r√©sumer un article
  python main.py --mode summarize --article "Chimie"
  
  # 5. Mode interactif
  python main.py --mode interactive
  
  # 6. D√©monstration avec articles pr√©d√©finis
  python main.py --mode demo
  
  # 7. Visualiser le preprocessing
  python main.py --mode visualize --article "Python (langage)"
  
  # 8. √âvaluer les m√©triques du pipeline
  python main.py --mode evaluate
        """
    )
    
    parser.add_argument(
        "--mode", 
        choices=[
            "build_dataset", "train", "summarize", "interactive", 
            "demo", "visualize", "install_spacy", "evaluate"
        ],
        default="interactive",
        help="Mode d'ex√©cution du programme"
    )
    
    parser.add_argument(
        "--article",
        type=str,
        help="Titre de l'article Wikipedia √† r√©sumer ou analyser"
    )
    
    parser.add_argument(
        "--num_articles",
        type=int,
        default=100,
        help="Nombre d'articles pour construire le dataset (mode build_dataset)"
    )
    
    parser.add_argument(
        "--lang",
        choices=["fr", "en"],
        default="fr",
        help="Langue des articles Wikipedia"
    )
    
    parser.add_argument(
        "--max_length",
        type=int,
        default=128,
        help="Longueur maximale du r√©sum√© g√©n√©r√©"
    )
    
    parser.add_argument(
        "--min_length",
        type=int,
        default=30,
        help="Longueur minimale du r√©sum√© g√©n√©r√©"
    )
    
    # Nouveaux param√®tres pour le mode autoencoder_clustering
    parser.add_argument(
        "--max_features",
        type=int,
        default=5000,
        help="Nombre maximum de features TF-IDF (mode train)"
    )
    
    parser.add_argument(
        "--encoding_dim",
        type=int,
        default=32,
        help="Dimensions de la couche d'encodage de l'auto-encodeur (mode train)"
    )
    
    parser.add_argument(
        "--n_clusters",
        type=int,
        default=5,
        help="Nombre de clusters pour K-means (mode train)"
    )
    
    parser.add_argument(
        "--epochs",
        type=int,
        default=100,
        help="Nombre d'√©poques d'entra√Ænement (mode train)"
    )
    
    args = parser.parse_args()
    
    print("=" * 80)
    print("ü§ñ R√âSUMEUR D'ARTICLES WIKIPEDIA - PIPELINE NLP COMPLET")
    print("=" * 80)
    print(f"üîß Mode s√©lectionn√©: {args.mode}")
    print(f"üåç Langue: {args.lang}")
    if args.article:
        print(f"üìÑ Article: {args.article}")
    print("=" * 80)
    
    try:
        # Dispatch vers la fonction appropri√©e selon le mode
        if args.mode == "build_dataset":
            build_dataset_mode(args)
            
        elif args.mode == "train":
            train_mode(args)
            
        elif args.mode == "summarize":
            summarize_mode(args)
            
        elif args.mode == "interactive":
            interactive_mode(args)
            
        elif args.mode == "demo":
            demo_mode(args)
            
        elif args.mode == "visualize":
            visualize_mode(args)
            
        elif args.mode == "install_spacy":
            install_spacy_mode(args)
            
        elif args.mode == "evaluate":
            evaluate_mode(args)
            

            
    except KeyboardInterrupt:
        print("\n\n‚èπÔ∏è  Interruption utilisateur. Au revoir! üëã")
        
    except Exception as e:
        logger.error(f"Erreur: {e}")
        print(f"\n‚ùå Erreur: {e}")
        print("\nüí° Conseil: V√©rifiez les pr√©requis et que les mod√®les sont bien install√©s.")
        raise


if __name__ == "__main__":
    main() 