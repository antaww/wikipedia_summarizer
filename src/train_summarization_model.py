#!/usr/bin/env python3
"""
Script d'entra√Ænement d'un mod√®le de r√©sum√© automatique
Utilise le dataset Wikipedia fran√ßais pour entra√Æner un mod√®le Transformer
"""

import os
import pandas as pd
import numpy as np
from datetime import datetime
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer, 
    AutoModelForSeq2SeqLM,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    DataCollatorForSeq2Seq
)
from sklearn.model_selection import train_test_split
import logging

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class WikipediaSummarizationDataset(Dataset):
    """
    Dataset personnalis√© pour la t√¢che de r√©sum√© Wikipedia
    """
    
    def __init__(self, texts, summaries, tokenizer, max_input_length=512, max_target_length=128):
        self.texts = texts
        self.summaries = summaries
        self.tokenizer = tokenizer
        self.max_input_length = max_input_length
        self.max_target_length = max_target_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        summary = str(self.summaries[idx])
        
        # Tokenisation du texte d'entr√©e
        inputs = self.tokenizer(
            text,
            max_length=self.max_input_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        # Tokenisation du r√©sum√© cible
        targets = self.tokenizer(
            summary,
            max_length=self.max_target_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        return {
            'input_ids': inputs['input_ids'].flatten(),
            'attention_mask': inputs['attention_mask'].flatten(),
            'labels': targets['input_ids'].flatten()
        }

def load_and_prepare_data(dataset_path, test_size=0.2, random_state=42):
    """
    Charge et pr√©pare les donn√©es pour l'entra√Ænement
    """
    logger.info(f"üìä Chargement du dataset: {dataset_path}")
    
    # Chargement du dataset
    df = pd.read_csv(dataset_path)
    logger.info(f"‚úÖ Dataset charg√©: {len(df)} √©chantillons")
    
    # Nettoyage des donn√©es manquantes
    df = df.dropna(subset=['original_content', 'wikipedia_summary'])
    logger.info(f"üßπ Apr√®s nettoyage: {len(df)} √©chantillons")
    
    # Extraction des colonnes n√©cessaires
    texts = df['original_content'].tolist()
    summaries = df['wikipedia_summary'].tolist()
    
    # Division train/test
    train_texts, test_texts, train_summaries, test_summaries = train_test_split(
        texts, summaries, test_size=test_size, random_state=random_state
    )
    
    logger.info(f"üìà Division des donn√©es:")
    logger.info(f"  - Entra√Ænement: {len(train_texts)} √©chantillons")
    logger.info(f"  - Test: {len(test_texts)} √©chantillons")
    
    return train_texts, test_texts, train_summaries, test_summaries

def setup_model_and_tokenizer(model_name="moussaKam/barthez"):
    """
    Configure le mod√®le et le tokenizer
    """
    logger.info(f"ü§ñ Chargement du mod√®le: {model_name}")
    
    # Chargement du tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    # Si pas de token de fin de s√©quence, en ajouter un
    if tokenizer.eos_token is None:
        tokenizer.add_special_tokens({'eos_token': '</s>'})
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # Chargement du mod√®le
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
    
    # Redimensionner les embeddings si n√©cessaire
    model.resize_token_embeddings(len(tokenizer))
    
    logger.info(f"‚úÖ Mod√®le et tokenizer configur√©s")
    return model, tokenizer

def train_model(train_dataset, test_dataset, model, tokenizer, output_dir="./models/summarization"):
    """
    Entra√Æne le mod√®le de r√©sum√©
    """
    logger.info("üöÄ D√©but de l'entra√Ænement...")
    
    # Configuration des arguments d'entra√Ænement
    training_args = Seq2SeqTrainingArguments(
        output_dir=output_dir,
        eval_strategy="epoch",  # Chang√© de evaluation_strategy √† eval_strategy
        learning_rate=3e-5,
        per_device_train_batch_size=4,
        per_device_eval_batch_size=4,
        weight_decay=0.01,
        save_total_limit=3,
        num_train_epochs=3,
        predict_with_generate=True,
        fp16=torch.cuda.is_available(),  # Utilise FP16 si GPU disponible
        push_to_hub=False,
        logging_dir=f"{output_dir}/logs",
        logging_steps=10,
        save_strategy="epoch",
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        report_to=None  # D√©sactive wandb/tensorboard
    )
    
    # Collator pour les donn√©es
    data_collator = DataCollatorForSeq2Seq(
        tokenizer=tokenizer,
        model=model,
        padding=True
    )
    
    # Configuration du trainer
    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        data_collator=data_collator,
        tokenizer=tokenizer
    )
    
    # Entra√Ænement
    logger.info("‚è≥ Entra√Ænement en cours...")
    trainer.train()
    
    # Sauvegarde du mod√®le final
    trainer.save_model()
    tokenizer.save_pretrained(output_dir)
    
    logger.info(f"‚úÖ Mod√®le sauvegard√© dans: {output_dir}")
    
    return trainer

def evaluate_model(trainer, test_dataset, tokenizer):
    """
    √âvalue le mod√®le sur le dataset de test
    """
    logger.info("üìä √âvaluation du mod√®le...")
    
    # √âvaluation
    eval_results = trainer.evaluate()
    
    logger.info("üìà R√©sultats d'√©valuation:")
    for key, value in eval_results.items():
        logger.info(f"  - {key}: {value:.4f}")
    
    return eval_results

def test_model_inference(model, tokenizer, test_text, max_length=128):
    """
    Teste l'inf√©rence du mod√®le sur un exemple
    """
    logger.info("üß™ Test d'inf√©rence...")
    
    # Tokenisation
    inputs = tokenizer(
        test_text,
        max_length=512,
        padding='max_length',
        truncation=True,
        return_tensors='pt'
    )
    
    # G√©n√©ration
    with torch.no_grad():
        summary_ids = model.generate(
            inputs['input_ids'],
            attention_mask=inputs['attention_mask'],
            max_length=max_length,
            num_beams=4,
            early_stopping=True
        )
    
    # D√©codage
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    
    logger.info("üìù Exemple de r√©sum√© g√©n√©r√©:")
    logger.info(f"Texte original (extrait): {test_text[:200]}...")
    logger.info(f"R√©sum√© g√©n√©r√©: {summary}")
    
    return summary

def train_summarization_model():
    """
    Fonction principale d'entra√Ænement
    """
    print("=== Entra√Ænement Mod√®le de R√©sum√© Wikipedia ===\n")
    
    # Configuration
    dataset_path = "data/wikipedia_dataset_fr.csv"
    output_dir = "./models/summarization_wikipedia"
    model_name = "moussaKam/barthez"  # Mod√®le BART fran√ßais pour r√©sum√©
    
    # V√©rification de l'existence du dataset
    if not os.path.exists(dataset_path):
        logger.error(f"‚ùå Dataset non trouv√©: {dataset_path}")
        return
    
    # Cr√©ation du dossier de sortie
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs("logs", exist_ok=True)
    
    try:
        # 1. Chargement et pr√©paration des donn√©es
        train_texts, test_texts, train_summaries, test_summaries = load_and_prepare_data(dataset_path)
        
        # 2. Configuration du mod√®le et tokenizer
        model, tokenizer = setup_model_and_tokenizer(model_name)
        
        # 3. Cr√©ation des datasets
        logger.info("üì¶ Cr√©ation des datasets...")
        train_dataset = WikipediaSummarizationDataset(
            train_texts, train_summaries, tokenizer
        )
        test_dataset = WikipediaSummarizationDataset(
            test_texts, test_summaries, tokenizer
        )
        
        # 4. Entra√Ænement
        trainer = train_model(train_dataset, test_dataset, model, tokenizer, output_dir)
        
        # 5. √âvaluation
        eval_results = evaluate_model(trainer, test_dataset, tokenizer)
        
        # 6. Test d'inf√©rence
        if test_texts:
            test_model_inference(model, tokenizer, test_texts[0])
        
        # 7. Sauvegarde des r√©sultats
        results_file = f"{output_dir}/training_results.txt"
        with open(results_file, "w", encoding="utf-8") as f:
            f.write(f"R√©sultats d'entra√Ænement - {datetime.now()}\n")
            f.write("="*50 + "\n\n")
            f.write(f"Mod√®le utilis√©: {model_name}\n")
            f.write(f"Dataset: {dataset_path}\n")
            f.write(f"√âchantillons d'entra√Ænement: {len(train_texts)}\n")
            f.write(f"√âchantillons de test: {len(test_texts)}\n\n")
            f.write("R√©sultats d'√©valuation:\n")
            for key, value in eval_results.items():
                f.write(f"  - {key}: {value:.4f}\n")
        
        logger.info(f"‚úÖ Entra√Ænement termin√©! R√©sultats sauv√©s dans: {results_file}")
        
    except Exception as e:
        logger.error(f"‚ùå Erreur durant l'entra√Ænement: {e}")
        raise

if __name__ == "__main__":
    try:
        train_summarization_model()
    except KeyboardInterrupt:
        logger.info("\n\nEntra√Ænement interrompu par l'utilisateur. Au revoir! üëã")
    except Exception as e:
        logger.error(f"‚ùå Erreur: {e}") 