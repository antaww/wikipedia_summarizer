#!/usr/bin/env python3
"""
Script de visualisation de la pipeline de preprocessing
RÃ©cupÃ¨re un article Wikipedia et montre chaque Ã©tape de preprocessing
"""

import sys
import logging
from pathlib import Path
import wikipediaapi
from nltk.tokenize import word_tokenize, sent_tokenize
import re
import string

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

# Import depuis le mÃªme dossier
from .config import Config
from .text_preprocessor import TextPreprocessor

def print_section(title: str, content: str, max_chars: int = 500):
    """Affiche une section avec un titre et du contenu limitÃ©"""
    print(f"\n{'=' * 60}")
    print(f"ğŸ“‹ {title}")
    print(f"{'=' * 60}")
    
    if len(content) > max_chars:
        print(f"{content[:max_chars]}...")
        print(f"\n[TronquÃ© - Longueur totale: {len(content)} caractÃ¨res]")
    else:
        print(content)
    
    print(f"\nğŸ“Š Statistiques: {len(content)} caractÃ¨res, {len(content.split())} mots")


def print_comparison(before: str, after: str, step_name: str, max_chars: int = 300):
    """Affiche une comparaison avant/aprÃ¨s pour une Ã©tape"""
    print(f"\n{'ğŸ”„ ' + step_name:-^60}")
    
    print(f"\nğŸ”´ AVANT ({len(before)} caractÃ¨res):")
    if len(before) > max_chars:
        print(f"{before[:max_chars]}...")
    else:
        print(before)
    
    print(f"\nğŸŸ¢ APRÃˆS ({len(after)} caractÃ¨res):")
    if len(after) > max_chars:
        print(f"{after[:max_chars]}...")
    else:
        print(after)
    
    # Statistiques de changement
    removed_chars = len(before) - len(after)
    if removed_chars > 0:
        reduction_percent = (removed_chars / len(before)) * 100
        print(f"\nğŸ“‰ RÃ©duction: -{removed_chars} caractÃ¨res (-{reduction_percent:.1f}%)")
    else:
        print(f"\nğŸ“ˆ Modification: {abs(removed_chars)} caractÃ¨res")


def get_wikipedia_article(title: str, language: str = "fr") -> dict:
    """RÃ©cupÃ¨re un article Wikipedia"""
    print(f"ğŸŒ RÃ©cupÃ©ration de l'article Wikipedia: '{title}' (langue: {language})")
    
    # Initialiser l'API Wikipedia
    wiki = wikipediaapi.Wikipedia('NLP-Preprocessing-Demo (demo@example.com)', language)
    
    page = wiki.page(title)
    
    if not page.exists():
        raise ValueError(f"âŒ Article '{title}' non trouvÃ© en {language}")
    
    print(f"âœ… Article trouvÃ©: {page.title}")
    print(f"ğŸ”— URL: {page.fullurl}")
    
    return {
        'title': page.title,
        'url': page.fullurl,
        'content': page.text,
        'summary': page.summary,
        'language': language
    }


def demonstrate_step_by_step_preprocessing(text: str, language: str = "fr"):
    """DÃ©montre chaque Ã©tape du preprocessing individuellement"""
    
    print(f"\n{'ğŸ”¬ ANALYSE Ã‰TAPE PAR Ã‰TAPE':-^80}")
    
    # Configuration de preprocessing pour permettre chaque Ã©tape
    config = Config()
    
    # Ã‰tape 1: Nettoyage des rÃ©fÃ©rences Wikipedia
    step1_text = re.sub(r'\[\d+\]', '', text)
    print_comparison(text[:500], step1_text[:500], "Ã‰TAPE 1: Suppression des rÃ©fÃ©rences [1], [2], etc.")
    
    # Ã‰tape 2: Suppression des balises Wikipedia
    step2_text = re.sub(r'{{[^}]*}}', '', step1_text)
    step2_text = re.sub(r'{[^}]*}', '', step2_text)
    print_comparison(step1_text[:500], step2_text[:500], "Ã‰TAPE 2: Suppression des balises Wikipedia")
    
    # Ã‰tape 3: Suppression des URLs
    step3_text = re.sub(r'(https?://\S+|www\.\S+|\S+\.(com|org|net|fr|en)\S*)', ' URL_TOKEN ', step2_text)
    print_comparison(step2_text[:500], step3_text[:500], "Ã‰TAPE 3: Remplacement des URLs")
    
    # Ã‰tape 4: Conversion en minuscules
    step4_text = step3_text.lower()
    print_comparison(step3_text[:500], step4_text[:500], "Ã‰TAPE 4: Conversion en minuscules")
    
    # Ã‰tape 5: Suppression de la ponctuation
    step5_text = step4_text.translate(str.maketrans('', '', string.punctuation))
    print_comparison(step4_text[:500], step5_text[:500], "Ã‰TAPE 5: Suppression de la ponctuation")
    
    # Ã‰tape 6: Nettoyage des espaces multiples
    step6_text = re.sub(r'\s+', ' ', step5_text).strip()
    print_comparison(step5_text[:500], step6_text[:500], "Ã‰TAPE 6: Nettoyage des espaces")
    
    # Ã‰tape 7: Tokenisation
    tokens = word_tokenize(step6_text)
    print(f"\n{'ğŸ”„ Ã‰TAPE 7: Tokenisation':-^60}")
    print(f"ğŸ”´ AVANT: Texte continu")
    print(f"ğŸŸ¢ APRÃˆS: {len(tokens)} tokens")
    print(f"ğŸ‘€ Premiers 20 tokens: {tokens[:20]}")
    
    # Ã‰tape 8: Suppression des stopwords
    processor = TextPreprocessor(config.PREPROCESSING_CONFIG)
    if language == "fr":
        stop_words = processor.stop_words_fr - processor.important_words_fr
    else:
        stop_words = processor.stop_words_en - processor.important_words_en
    
    filtered_tokens = [word for word in tokens if word not in stop_words and word.isalpha()]
    
    print(f"\n{'ğŸ”„ Ã‰TAPE 8: Suppression des stopwords':-^60}")
    print(f"ğŸ”´ AVANT: {len(tokens)} tokens")
    print(f"ğŸŸ¢ APRÃˆS: {len(filtered_tokens)} tokens")
    print(f"ğŸ“‰ SupprimÃ©s: {len(tokens) - len(filtered_tokens)} stopwords")
    print(f"ğŸ‘€ Premiers 20 tokens filtrÃ©s: {filtered_tokens[:20]}")
    
    # Ã‰tape 9: Lemmatisation
    from textblob import TextBlob
    text_for_lemma = ' '.join(filtered_tokens)
    blob = TextBlob(text_for_lemma)
    lemmatized_words = [word.lemmatize() for word in blob.words]
    
    print(f"\n{'ğŸ”„ Ã‰TAPE 9: Lemmatisation':-^60}")
    print(f"ğŸ”´ AVANT: {filtered_tokens[:10]}")
    print(f"ğŸŸ¢ APRÃˆS: {lemmatized_words[:10]}")
    
    # Texte final
    final_text = ' '.join(lemmatized_words)
    
    return final_text


def demonstrate_full_pipeline(text: str, language: str = "fr"):
    """DÃ©montre la pipeline complÃ¨te via TextPreprocessor"""
    print(f"\n{'ğŸš€ PIPELINE COMPLÃˆTE':-^80}")
    
    config = Config()
    processor = TextPreprocessor(config.PREPROCESSING_CONFIG)
    
    # Pipeline complÃ¨te
    clean_text, features = processor.preprocessing_pipeline(text, language)
    
    print_section("TEXTE ORIGINAL", text)
    print_section("TEXTE APRÃˆS PIPELINE COMPLÃˆTE", clean_text)
    
    # Affichage des features extraites
    print(f"\n{'ğŸ“Š FEATURES EXTRAITES':-^60}")
    for feature, value in features.items():
        if isinstance(value, float):
            print(f"ğŸ“ˆ {feature}: {value:.3f}")
        else:
            print(f"ğŸ“ˆ {feature}: {value}")


def visualize_preprocessing(article_title: str = None, language: str = "fr"):
    """Fonction principale de visualisation"""
    print("ğŸ¯ VISUALISATION DE LA PIPELINE DE PREPROCESSING")
    print("=" * 80)
    
    # ParamÃ¨tres par dÃ©faut
    default_article = "Intelligence artificielle"
    default_language = "fr"
    
    if not article_title:
        # Demander Ã  l'utilisateur de choisir un article
        print(f"\nArticle par dÃ©faut: '{default_article}' (langue: {default_language})")
        choice = input("ğŸ“ Appuyez sur EntrÃ©e pour continuer ou tapez un autre titre d'article: ").strip()
        
        if choice:
            article_title = choice
        else:
            article_title = default_article
        
        # Choix de la langue
        lang_choice = input("ğŸŒ Langue (fr/en) [fr]: ").strip().lower()
        language = lang_choice if lang_choice in ['fr', 'en'] else default_language
    
    try:
        # RÃ©cupÃ©ration de l'article
        article = get_wikipedia_article(article_title, language)
        
        # Limitation du texte pour la dÃ©monstration
        text = article['content'][:3000]  # Premiers 3000 caractÃ¨res
        
        print(f"\nğŸ“„ Article sÃ©lectionnÃ©: {article['title']}")
        print(f"ğŸ“ Texte analysÃ©: {len(text)} caractÃ¨res (limitÃ© pour la dÃ©monstration)")
        
        # DÃ©monstration Ã©tape par Ã©tape
        final_text = demonstrate_step_by_step_preprocessing(text, language)
        
        # DÃ©monstration de la pipeline complÃ¨te
        demonstrate_full_pipeline(text, language)
        
        # RÃ©sumÃ© final
        print(f"\n{'âœ… RÃ‰SUMÃ‰ FINAL':-^80}")
        print(f"ğŸ“Š Texte original: {len(text)} caractÃ¨res")
        print(f"ğŸ“Š Texte final: {len(final_text)} caractÃ¨res") 
        print(f"ğŸ“‰ RÃ©duction totale: {((len(text) - len(final_text)) / len(text) * 100):.1f}%")
        
        print(f"\nğŸ‰ DÃ©monstration terminÃ©e avec succÃ¨s !")
        
    except Exception as e:
        logger.error(f"âŒ Erreur: {e}")
        raise


if __name__ == "__main__":
    visualize_preprocessing() 